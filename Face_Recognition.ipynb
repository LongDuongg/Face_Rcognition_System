{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import matplotlib.pyplot as plt \n",
    "import albumentations as alb\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coordinate(label_path) :\n",
    "  with open(label_path, 'rb') as file :\n",
    "    label = json.load(file)\n",
    "    \n",
    "  return [\n",
    "    int(math.ceil(label[\"shapes\"][0]['points'][0][0])),\n",
    "    int(math.ceil(label[\"shapes\"][0]['points'][0][1])),\n",
    "    int(math.ceil(label[\"shapes\"][0]['points'][1][0])),\n",
    "    int(math.ceil(label[\"shapes\"][0]['points'][1][1])),\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop face on image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(folder_in, folder_out, coordinates, label) :\n",
    "  curentImg = cv2.imread(folder_in)\n",
    "  curentImgRGB = cv2.cvtColor(curentImg, cv2.COLOR_BGR2RGB)    \n",
    "  \n",
    "  cv2.imwrite(\n",
    "    os.path.join('Cropped_Face_Images', folder_out, 'Quoc', label.split('.')[0] + '.jpg'), \n",
    "    cv2.resize(\n",
    "      curentImgRGB[coordinates[1] : coordinates[3], coordinates[0] : coordinates[2]],\n",
    "      (120,120)\n",
    "    )\n",
    "  )\n",
    "  \n",
    "  print(\"Processing : \", label.split('.')[0] + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in ['Data_Train', 'Data_Test', 'Data_Validation'] :\n",
    "  for label_file in os.listdir(os.path.join(folder, 'Quoc_Label')) :\n",
    "    coordinates = load_coordinate(os.path.join(folder, 'Quoc_Label', label_file)) \n",
    "    folder_in = os.path.join(folder, 'Quoc', label_file.split('.')[0] + '.jpg')\n",
    "    \n",
    "    preprocess(folder_in, folder, coordinates, label_file)\n",
    "    time.sleep(2)\n",
    "print('finish process !!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = alb.Compose([\n",
    "  alb.HorizontalFlip(p=0.5),\n",
    "  alb.VerticalFlip(p=0.5),\n",
    "  alb.RandomBrightnessContrast(p=0.2),\n",
    "  alb.RandomGamma(p=0.2),\n",
    "  alb.RGBShift(p=0.2),],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in ['Data_Train', 'Data_Test', 'Data_Validation'] :\n",
    "  for sub_folder, sub_folder2 in zip(['Long', 'Phuc', 'Quoc'], ['Long_Augmented', 'Phuc_Augmented', 'Quoc_Augmented']) :\n",
    "    for image_file in os.listdir(os.path.join('Cropped_Face_Images', folder, sub_folder)) :\n",
    "      image = cv2.imread(os.path.join('Cropped_Face_Images', folder, sub_folder, image_file))\n",
    "        \n",
    "      try:\n",
    "        for x in range(10) :\n",
    "          augmented = augmentor(image=image)\n",
    "          cv2.imwrite(os.path.join('Cropped_Face_Images', folder, sub_folder2, f'{image_file.split(\".\")[0]}.{x}.jpg'), augmented['image'])\n",
    "      except Exception as e :\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "  rescale=1./255,\n",
    "  shear_range=0.1,\n",
    "  zoom_range=0.1,\n",
    "  horizontal_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "  rescale=1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(\n",
    "  rescale=1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6290 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "data_train = train_datagen.flow_from_directory(\n",
    "  'Cropped_Face_Images\\\\Data_Train',\n",
    "  target_size=(120, 120),\n",
    "  batch_size=32,\n",
    "  class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1550 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "data_test = test_datagen.flow_from_directory(\n",
    "  'Cropped_Face_Images\\\\Data_Test',\n",
    "  target_size=(120, 120),\n",
    "  batch_size=32,\n",
    "  class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1540 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "data_val = val_datagen.flow_from_directory(\n",
    "  'Cropped_Face_Images\\\\Data_Validation',\n",
    "  target_size=(120, 120),\n",
    "  batch_size=32,\n",
    "  class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a mapping for index and face names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_indices have the numeric tag for each face\n",
    "train_classes = data_train.class_indices \n",
    "\n",
    "# Storing the face and the numeric tag for future reference\n",
    "ResultMap={}\n",
    "for faceValue,faceName in zip(train_classes.values(),train_classes.keys()):\n",
    "    ResultMap[faceValue]=faceName\n",
    "    \n",
    "# Saving the face map for future reference\n",
    "# with open(\"ResultsMap.pkl\", 'wb') as fileWriteStream:\n",
    "#     pickle.dump(ResultMap, fileWriteStream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the CNN face recognition model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Convolution2D, MaxPool2D, Flatten, Dense, BatchNormalization\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.add(Convolution2D(32, kernel_size=(3,3), strides=(1,1), input_shape=(120,120,3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D())\n",
    " \n",
    "model.add(Convolution2D(64, kernel_size=(3,3), strides=(1,1), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "# model.add(Convolution2D(16, kernel_size=(3,3), strides=(1,1), activation='relu'))\n",
    "# model.add(MaxPool2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=64,activation=\"relu\"))\n",
    "model.add(Dense(units=3, activation=\"softmax\"))\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 118, 118, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 118, 118, 32)      128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 59, 59, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 57, 57, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 57, 57, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 28, 28, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 50176)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                3211328   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3231299 (12.33 MB)\n",
      "Trainable params: 3231107 (12.33 MB)\n",
      "Non-trainable params: 192 (768.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = 'recognition_logs'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "30/30 [==============================] - 21s 584ms/step - loss: 3.2782 - accuracy: 0.6771 - val_loss: 2.5686 - val_accuracy: 0.2188\n",
      "Epoch 2/40\n",
      "30/30 [==============================] - 18s 604ms/step - loss: 0.8441 - accuracy: 0.8125 - val_loss: 9.7631 - val_accuracy: 0.2281\n",
      "Epoch 3/40\n",
      "30/30 [==============================] - 17s 562ms/step - loss: 0.4423 - accuracy: 0.8781 - val_loss: 12.3984 - val_accuracy: 0.2281\n",
      "Epoch 4/40\n",
      "30/30 [==============================] - 17s 570ms/step - loss: 0.3662 - accuracy: 0.8979 - val_loss: 14.5203 - val_accuracy: 0.2906\n",
      "Epoch 5/40\n",
      "30/30 [==============================] - 18s 594ms/step - loss: 0.4373 - accuracy: 0.8885 - val_loss: 27.9776 - val_accuracy: 0.2344\n",
      "Epoch 6/40\n",
      "30/30 [==============================] - 18s 604ms/step - loss: 0.5388 - accuracy: 0.8875 - val_loss: 16.0650 - val_accuracy: 0.2500\n",
      "Epoch 7/40\n",
      "30/30 [==============================] - 18s 582ms/step - loss: 0.4370 - accuracy: 0.9112 - val_loss: 29.5636 - val_accuracy: 0.2313\n",
      "Epoch 8/40\n",
      "30/30 [==============================] - 19s 622ms/step - loss: 0.3068 - accuracy: 0.9250 - val_loss: 31.0633 - val_accuracy: 0.2094\n",
      "Epoch 9/40\n",
      "30/30 [==============================] - 19s 620ms/step - loss: 0.3879 - accuracy: 0.9187 - val_loss: 15.8616 - val_accuracy: 0.3000\n",
      "Epoch 10/40\n",
      "30/30 [==============================] - 18s 592ms/step - loss: 0.3559 - accuracy: 0.9333 - val_loss: 24.0407 - val_accuracy: 0.2406\n",
      "Epoch 11/40\n",
      "30/30 [==============================] - 18s 587ms/step - loss: 0.2020 - accuracy: 0.9542 - val_loss: 35.8571 - val_accuracy: 0.2594\n",
      "Epoch 12/40\n",
      "30/30 [==============================] - 18s 594ms/step - loss: 0.1141 - accuracy: 0.9719 - val_loss: 30.5524 - val_accuracy: 0.2344\n",
      "Epoch 13/40\n",
      "30/30 [==============================] - 20s 676ms/step - loss: 0.1088 - accuracy: 0.9802 - val_loss: 15.0317 - val_accuracy: 0.3344\n",
      "Epoch 14/40\n",
      "30/30 [==============================] - 18s 591ms/step - loss: 0.0574 - accuracy: 0.9833 - val_loss: 5.3758 - val_accuracy: 0.5813\n",
      "Epoch 15/40\n",
      "30/30 [==============================] - 18s 602ms/step - loss: 0.1149 - accuracy: 0.9760 - val_loss: 3.2470 - val_accuracy: 0.6844\n",
      "Epoch 16/40\n",
      "30/30 [==============================] - 18s 591ms/step - loss: 0.1061 - accuracy: 0.9729 - val_loss: 1.3956 - val_accuracy: 0.7500\n",
      "Epoch 17/40\n",
      "30/30 [==============================] - 18s 576ms/step - loss: 0.1229 - accuracy: 0.9781 - val_loss: 1.0662 - val_accuracy: 0.8094\n",
      "Epoch 18/40\n",
      "30/30 [==============================] - 19s 617ms/step - loss: 0.0618 - accuracy: 0.9831 - val_loss: 1.3524 - val_accuracy: 0.8125\n",
      "Epoch 19/40\n",
      "30/30 [==============================] - 18s 600ms/step - loss: 0.1912 - accuracy: 0.9802 - val_loss: 0.5745 - val_accuracy: 0.8906\n",
      "Epoch 20/40\n",
      "30/30 [==============================] - 18s 609ms/step - loss: 0.2559 - accuracy: 0.9510 - val_loss: 2.7536 - val_accuracy: 0.7781\n",
      "Epoch 21/40\n",
      "30/30 [==============================] - 18s 582ms/step - loss: 0.1739 - accuracy: 0.9672 - val_loss: 0.2544 - val_accuracy: 0.9281\n",
      "Epoch 22/40\n",
      "30/30 [==============================] - 17s 575ms/step - loss: 0.0706 - accuracy: 0.9812 - val_loss: 0.0694 - val_accuracy: 0.9719\n",
      "Epoch 23/40\n",
      "30/30 [==============================] - 16s 530ms/step - loss: 0.0267 - accuracy: 0.9906 - val_loss: 0.1732 - val_accuracy: 0.9531\n",
      "Epoch 24/40\n",
      "30/30 [==============================] - 13s 423ms/step - loss: 0.0498 - accuracy: 0.9854 - val_loss: 0.1265 - val_accuracy: 0.9563\n",
      "Epoch 25/40\n",
      "30/30 [==============================] - 13s 417ms/step - loss: 0.0304 - accuracy: 0.9917 - val_loss: 0.2763 - val_accuracy: 0.9344\n",
      "Epoch 26/40\n",
      "30/30 [==============================] - 13s 416ms/step - loss: 0.0967 - accuracy: 0.9781 - val_loss: 0.2479 - val_accuracy: 0.9281\n",
      "Epoch 27/40\n",
      "30/30 [==============================] - 13s 419ms/step - loss: 0.0530 - accuracy: 0.9831 - val_loss: 0.1123 - val_accuracy: 0.9688\n",
      "Epoch 28/40\n",
      "30/30 [==============================] - 13s 426ms/step - loss: 0.0491 - accuracy: 0.9844 - val_loss: 0.0133 - val_accuracy: 0.9969\n",
      "Epoch 29/40\n",
      "30/30 [==============================] - 13s 414ms/step - loss: 0.0234 - accuracy: 0.9937 - val_loss: 0.1539 - val_accuracy: 0.9750\n",
      "Epoch 30/40\n",
      "30/30 [==============================] - 13s 413ms/step - loss: 0.0644 - accuracy: 0.9896 - val_loss: 0.5300 - val_accuracy: 0.9531\n",
      "Epoch 31/40\n",
      "30/30 [==============================] - 12s 412ms/step - loss: 0.0506 - accuracy: 0.9885 - val_loss: 0.1625 - val_accuracy: 0.9500\n",
      "Epoch 32/40\n",
      "30/30 [==============================] - 13s 413ms/step - loss: 0.0506 - accuracy: 0.9854 - val_loss: 0.4960 - val_accuracy: 0.9469\n",
      "Epoch 33/40\n",
      "30/30 [==============================] - 13s 415ms/step - loss: 0.0644 - accuracy: 0.9852 - val_loss: 1.0155 - val_accuracy: 0.9406\n",
      "Epoch 34/40\n",
      "30/30 [==============================] - 12s 409ms/step - loss: 0.0572 - accuracy: 0.9820 - val_loss: 0.1811 - val_accuracy: 0.9594\n",
      "Epoch 35/40\n",
      "30/30 [==============================] - 13s 445ms/step - loss: 0.0682 - accuracy: 0.9927 - val_loss: 0.4213 - val_accuracy: 0.9219\n",
      "Epoch 36/40\n",
      "30/30 [==============================] - 13s 427ms/step - loss: 0.0907 - accuracy: 0.9844 - val_loss: 0.7176 - val_accuracy: 0.8875\n",
      "Epoch 37/40\n",
      "30/30 [==============================] - 13s 427ms/step - loss: 0.1687 - accuracy: 0.9820 - val_loss: 0.6235 - val_accuracy: 0.9156\n",
      "Epoch 38/40\n",
      "30/30 [==============================] - 13s 416ms/step - loss: 0.0587 - accuracy: 0.9823 - val_loss: 0.3006 - val_accuracy: 0.9594\n",
      "Epoch 39/40\n",
      "30/30 [==============================] - 13s 414ms/step - loss: 0.0529 - accuracy: 0.9820 - val_loss: 0.9945 - val_accuracy: 0.8969\n",
      "Epoch 40/40\n",
      "30/30 [==============================] - 14s 444ms/step - loss: 0.0769 - accuracy: 0.9885 - val_loss: 0.2964 - val_accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(data_train, steps_per_epoch=30, epochs=40, validation_data = data_val, validation_steps=10, callbacks = [tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols = 2, figsize = (20,5))\n",
    "\n",
    "ax[0].plot(hist.history['loss'], color = 'teal', label = 'loss')\n",
    "ax[0].plot(hist.history['val_loss'], color = 'orange', label = 'val_loss')\n",
    "ax[0].title.set_text('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(hist.history['accuracy'], color = 'teal', label = 'accuracy')\n",
    "ax[1].plot(hist.history['val_accuracy'], color = 'orange', label = 'val_accuracy')\n",
    "ax[1].title.set_text('Accuracy')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('BatchNormalization.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_recognition = load_model('FaceRecognition.keras')\n",
    "\n",
    "with open(\"ResultsMap.pkl\", 'rb') as file:\n",
    "  result_map = pickle.load(file)\n",
    "\n",
    "test_image = cv2.imread('Phuc2.jpg')\n",
    "\n",
    "test_image = cv2.resize(test_image, (120,120))\n",
    " \n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    " \n",
    "result = face_recognition.predict(test_image,verbose=0) \n",
    " \n",
    "print('####'*10)\n",
    "print(result)\n",
    "print('Prediction is: ', result_map[np.argmax(result)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performFaceRecognition(vid) :\n",
    "  facetracker = load_model('Quoc.h5')\n",
    "  face_recognition = load_model('BatchNormalization.keras')\n",
    "  \n",
    "  with open(\"ResultsMap.pkl\", 'rb') as file:\n",
    "   result_map = pickle.load(file)\n",
    "  \n",
    "  \n",
    "  while True :\n",
    "    result, frame = vid.read()\n",
    "    if result is False:\n",
    "      break\n",
    "    frame = frame[50 : 500, 50 : 500,:]\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120,120))\n",
    "    \n",
    "    yhat = facetracker.predict(np.expand_dims(resized/255, 0))\n",
    "    print(yhat)\n",
    "    sample_coords = yhat[1][0]\n",
    "  \n",
    "    if yhat[0] > 0.5 :\n",
    "      # Control the main rectangle\n",
    "      cv2.rectangle(\n",
    "        frame,\n",
    "        tuple(np.multiply(sample_coords[:2], [450,450]).astype(int)),\n",
    "        tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)),\n",
    "        (255,0,0),\n",
    "        2 \n",
    "      )\n",
    "      \n",
    "      resize = cv2.resize(\n",
    "        frame[\n",
    "          np.multiply(sample_coords[1], 450).astype(int) : np.multiply(sample_coords[3], 450).astype(int),\n",
    "          np.multiply(sample_coords[0], 450).astype(int) : np.multiply(sample_coords[2], 450).astype(int)\n",
    "        ],  \n",
    "        (120,120)\n",
    "      )\n",
    "      \n",
    "      name = face_recognition.predict(np.expand_dims(resize/255, 0), verbose=0)\n",
    "\n",
    "      # Control the text rendered\n",
    "      cv2.putText(\n",
    "        frame, \n",
    "        result_map[np.argmax(name)],\n",
    "        tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),[0, -5])),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (255,255,255),\n",
    "        2,\n",
    "        cv2.LINE_AA\n",
    "      )\n",
    "      \n",
    "    cv2.imshow(\"My Face Detection Project\", frame) \n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "      break\n",
    "    \n",
    "  vid.release()\n",
    "  cv2.destroyAllWindows()\n",
    "  \n",
    "def accessCamera(IP_Stream) :\n",
    "  return cv2.VideoCapture(IP_Stream)\n",
    "\n",
    "# url = 'http://192.168.1.105'\n",
    "video_stream = accessCamera(0)\n",
    "performFaceRecognition(video_stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
